{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-based-method",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LclC-eroaxHm"
      },
      "source": [
        "# CADEC 데이터를 훈련시켜 NER 기반의 약물 탐지 모델 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cB1RBGTJTn7",
        "outputId": "ba7286ba-c064-4e4f-beb6-82f10c74695c"
      },
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "!pip install keras==2.2.4\n",
        "!pip install tensorflow-gpu==1.14.0\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-gpu==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.34.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (57.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.5.0)\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-58dgi6ar\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-58dgi6ar\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7Bat1hEJWD_"
      },
      "source": [
        "import re\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYyMJq0NJX15",
        "outputId": "37bbe559-c0f2-4c07-b242-b303f4902fd9"
      },
      "source": [
        "f = open('/content/drive/My Drive/test.txt', 'r')\n",
        "tagged_sentences = []\n",
        "sentence = []\n",
        "\n",
        "for line in f:\n",
        "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\t\":\n",
        "        if len(sentence) > 0:\n",
        "            tagged_sentences.append(sentence)\n",
        "            sentence = []\n",
        "        continue\n",
        "\n",
        "    splits = line.split('\t') \n",
        "    \n",
        "    splits[-1] = re.sub(r'\\n', '', splits[-1]) \n",
        "    word = splits[0].lower()\n",
        "    sentence.append([word, splits[-1]])\n",
        "\n",
        "print(\"전체 샘플 개수: \", len(tagged_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 샘플 개수:  7593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "8oR4qAOMJZ9f",
        "outputId": "97fda302-ba44-496e-d5f0-7e1bdbd1e37d"
      },
      "source": [
        "sentences, ner_tags = [], [] \n",
        "for tagged_sentence in tagged_sentences: \n",
        "    sentence, tag_info = zip(*tagged_sentence) \n",
        "    sentences.append(list(sentence)) \n",
        "    ner_tags.append(list(tag_info)) \n",
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 236\n",
            "샘플의 평균 길이 : 16.016199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz3PjDM5Jbnd",
        "outputId": "64731ca0-98bf-4d49-d242-ca400e9c757c"
      },
      "source": [
        "src_tokenizer = Tokenizer(oov_token='OOV') \n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "tar_tokenizer = Tokenizer(lower=False) \n",
        "tar_tokenizer.fit_on_texts(ner_tags)\n",
        "vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\n",
        "\n",
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(ner_tags)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 6757\n",
            "개체명 태깅 정보 집합의 크기 : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQDqXQKqJdPv"
      },
      "source": [
        "word_to_index = src_tokenizer.word_index\n",
        "index_to_word = src_tokenizer.index_word\n",
        "ner_to_index = tar_tokenizer.word_index\n",
        "index_to_ner = tar_tokenizer.index_word\n",
        "index_to_ner[0] = 'PAD'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJIOZFZkJf1y",
        "outputId": "072a3995-f010-4a35-b797-9f032266f32d"
      },
      "source": [
        "decoded = []\n",
        "for index in X_train[0] : \n",
        "    decoded.append(index_to_word[index])\n",
        "\n",
        "print('기존의 문장 : {}'.format(sentences[0]))\n",
        "print('디코딩 문장 : {}'.format(decoded))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "기존의 문장 : ['pain', 'in', 'my', 'left', 'leg', 'and', 'most', 'of', 'my', 'joints', '.']\n",
            "디코딩 문장 : ['pain', 'in', 'my', 'left', 'leg', 'and', 'most', 'of', 'my', 'joints', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZpR8aIOJhfg"
      },
      "source": [
        "max_len = 70\n",
        "max_len_char = 10\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTzwvvXgJj6I",
        "outputId": "b6a5efe1-c5ca-4875-fef4-40a71ebc5de2"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.3, random_state=777)\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)\n",
        "print(len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM7PNI2rJmbv",
        "outputId": "71777b91-53c8-48ca-8e98-eea22f17649c"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "class F1score(Callback):\n",
        "    def __init__(self, value = 0.0, use_char=True):\n",
        "        super(F1score, self).__init__()\n",
        "        self.value = value\n",
        "        self.use_char = use_char\n",
        "\n",
        "    def sequences_to_tags(self, sequences): \n",
        "      result = []\n",
        "      for sequence in sequences: \n",
        "          tag = []\n",
        "          for pred in sequence: \n",
        "              pred_index = np.argmax(pred) \n",
        "              tag.append(index_to_ner[pred_index].replace(\"PAD\", \"O\")) \n",
        "          result.append(tag)\n",
        "      return result\n",
        "\n",
        " \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "      # char Embedding을 사용하는 경우\n",
        "      if self.use_char:\n",
        "        X_test = self.validation_data[0]\n",
        "        X_char_test = self.validation_data[1]\n",
        "        y_test = self.validation_data[2]\n",
        "        y_predicted = self.model.predict([X_test, X_char_test])\n",
        "\n",
        "      else:\n",
        "        X_test = self.validation_data[0]\n",
        "        y_test = self.validation_data[1]\n",
        "        y_predicted = self.model.predict([X_test])\n",
        "\n",
        "      pred_tags = self.sequences_to_tags(y_predicted)\n",
        "      test_tags = self.sequences_to_tags(y_test)\n",
        "\n",
        "      score = f1_score(pred_tags, test_tags)\n",
        "      print(' - f1: {:04.2f}'.format(score * 100))\n",
        "      print(classification_report(test_tags, pred_tags))\n",
        "\n",
        "      \n",
        "      if score > self.value:\n",
        "        print('f1_score improved from %f to %f, saving model to best_model.h5'%(self.value, score))\n",
        "        self.model.save('/content/drive/My Drive/best_model.h5')\n",
        "        self.value = score\n",
        "      else:\n",
        "        print('f1_score did not improve from %f'%(self.value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovay2OS6lrQl",
        "outputId": "926d0099-301d-407e-a404-b1bde8820f06"
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-u1388bgx\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-u1388bgx\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Av1ZVgHJqhy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.python.keras.models import Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras_contrib.layers import CRF\n",
        "from pathlib import Path\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import SpatialDropout1D\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnC8JaB4JtwI",
        "outputId": "a2c81dd4-c1fe-454a-f07a-13cb0e8988ef"
      },
      "source": [
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/My Drive/glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1210 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24iXxQ1aJvNm",
        "outputId": "f6f95b86-4c0c-4efa-9851-dcd412257a80"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "word_index = src_tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "num_words = min(max_len, len(word_index)) + 1\n",
        "print(num_words)\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_index.items():\n",
        "    if i > max_len:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6756 unique tokens.\n",
            "71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSCo3kOvly2x",
        "outputId": "c79aa0ac-e67e-4d59-9325-a228bfcbeec8"
      },
      "source": [
        "from sklearn.utils import class_weight, compute_sample_weight,compute_class_weight\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from keras.models import load_model\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight, compute_sample_weight,compute_class_weight\n",
        "class_weights = np.zeros((70, 4))\n",
        "class_weights[:, 0] +=  0.383408827\n",
        "class_weights[:, 1] += 1\n",
        "class_weights[:, 2] += 6.829094379877604\n",
        "class_weights[:, 3] += 4.0752169397259355\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim,embeddings_initializer=Constant(embedding_matrix),input_length=max_len, mask_zero=True,trainable=True))\n",
        "\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(100, activation=\"relu\")))\n",
        "#model.add(Dropout(0.52))\n",
        "crf = CRF(tag_size)\n",
        "model.add(crf)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=4) \n",
        "adam =Adam(lr=0.001,decay=0.9)\n",
        "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "#history = model.fit(X_train, y_train, batch_size = 128, epochs = 25, validation_split = 0.1, verbose = 1, callbacks=[F1score(use_char=False)])\n",
        "model.summary()\n",
        "#history = model.fit(X_train, y_train, batch_size=20, epochs=200,validation_split=0.1, verbose=2,class_weight=class_weights)\n",
        "history = model.fit(X_train, y_train, batch_size=20, epochs=200,validation_split=0.1, verbose=2,callbacks = [es],class_weight=class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 70, 300)           2027100   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 70, 200)           320800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 70, 200)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 70, 100)           20100     \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 70, 4)             428       \n",
            "=================================================================\n",
            "Total params: 2,368,428\n",
            "Trainable params: 2,368,428\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 4783 samples, validate on 532 samples\n",
            "Epoch 1/200\n",
            " - 92s - loss: 8.5049 - crf_viterbi_accuracy: 0.8863 - val_loss: 8.6216 - val_crf_viterbi_accuracy: 0.9174\n",
            "Epoch 2/200\n",
            " - 85s - loss: 8.3726 - crf_viterbi_accuracy: 0.9297 - val_loss: 8.5975 - val_crf_viterbi_accuracy: 0.9276\n",
            "Epoch 3/200\n",
            " - 86s - loss: 8.3373 - crf_viterbi_accuracy: 0.9426 - val_loss: 8.5963 - val_crf_viterbi_accuracy: 0.9203\n",
            "Epoch 4/200\n",
            " - 87s - loss: 8.3207 - crf_viterbi_accuracy: 0.9483 - val_loss: 8.6056 - val_crf_viterbi_accuracy: 0.9296\n",
            "Epoch 5/200\n",
            " - 86s - loss: 8.3074 - crf_viterbi_accuracy: 0.9549 - val_loss: 8.6245 - val_crf_viterbi_accuracy: 0.9129\n",
            "Epoch 6/200\n",
            " - 85s - loss: 8.2976 - crf_viterbi_accuracy: 0.9606 - val_loss: 8.6288 - val_crf_viterbi_accuracy: 0.9038\n",
            "Epoch 7/200\n",
            " - 85s - loss: 8.2900 - crf_viterbi_accuracy: 0.9653 - val_loss: 8.6410 - val_crf_viterbi_accuracy: 0.9108\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdj0Ki0u6RHL",
        "outputId": "efdf85de-ba50-4a21-9107-bbc429c5e4e8"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "from sklearn.metrics import classification_report,f1_score\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "# Convert the index to tag\n",
        "\n",
        "\n",
        "report = flat_classification_report(y_pred=pred_tags, y_true=test_tags)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ADR       0.74      0.73      0.73      1763\n",
            "       I-ADR       0.59      0.74      0.66      2904\n",
            "           O       0.99      0.99      0.99    154793\n",
            "\n",
            "    accuracy                           0.98    159460\n",
            "   macro avg       0.78      0.82      0.79    159460\n",
            "weighted avg       0.98      0.98      0.98    159460\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWB6UmGza_sB"
      },
      "source": [
        "# celecoxib, ibuprofen, naproxen의 부정 리뷰를 모델에 적용해 약물 부작용 표현 추출 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx504vnHOFsq",
        "outputId": "4376fe50-25f7-4bb4-e00d-13205a2bc06c"
      },
      "source": [
        "f = open('/content/drive/My Drive/0802naproxen_neg.txt', 'r')\n",
        "tagged_sentences = []\n",
        "sentence = []\n",
        "\n",
        "for line in f:\n",
        "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\t\":\n",
        "        if len(sentence) > 0:\n",
        "            tagged_sentences.append(sentence)\n",
        "            sentence = []\n",
        "        continue\n",
        "\n",
        "    splits = line.split('\t') \n",
        "    \n",
        "    splits[-1] = re.sub(r'\\n', '', splits[-1]) \n",
        "    word = splits[0].lower() \n",
        "    sentence.append([word, splits[-1]]) \n",
        "\n",
        "print(\"전체 샘플 개수: \", len(tagged_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 샘플 개수:  1289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btiREQ2rOHnf"
      },
      "source": [
        "sentences, ner_tags = [], [] \n",
        "for tagged_sentence in tagged_sentences: \n",
        "    sentence, tag_info = zip(*tagged_sentence) \n",
        "    sentences.append(list(sentence)) \n",
        "    ner_tags.append(list(tag_info)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC8u-x7POKum",
        "outputId": "9a10e9d6-7989-44df-862a-9eba3f1a55dc"
      },
      "source": [
        "src_tokenizer = Tokenizer(oov_token='OOV') \n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "tar_tokenizer = Tokenizer(lower=False)\n",
        "tar_tokenizer.fit_on_texts(ner_tags)\n",
        "vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\n",
        "\n",
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(ner_tags)\n",
        "\n",
        "word_to_index = src_tokenizer.word_index\n",
        "index_to_word = src_tokenizer.index_word\n",
        "ner_to_index = tar_tokenizer.word_index\n",
        "index_to_ner = tar_tokenizer.index_word\n",
        "index_to_ner[0] = 'PAD'\n",
        "\n",
        "decoded = []\n",
        "for index in X_train[0] : \n",
        "    decoded.append(index_to_word[index]) \n",
        "\n",
        "\n",
        "max_len = 70\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=tag_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 2423\n",
            "개체명 태깅 정보 집합의 크기 : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swp7CfK_ONg1",
        "outputId": "aa90c5c3-2583-4c72-b8f0-0b31336a5cde"
      },
      "source": [
        "i=3 \n",
        "y_predicted =model.predict(np.array([X_train[i]])) \n",
        "y_predicted = np.argmax(y_predicted, axis=-1) \n",
        "true = np.argmax(y_train[i], -1) \n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for w, t, pred in zip(X_train[i], true, y_predicted[0]):\n",
        "    if w != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t], index_to_ner[pred]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어             |실제값  |예측값\n",
            "-----------------------------------\n",
            "i                : O       O\n",
            "took             : O       O\n",
            "the              : O       O\n",
            "naproxen         : O       O\n",
            "after            : O       O\n",
            "eating           : O       O\n",
            "some             : O       I-ADR\n",
            "food             : O       O\n",
            ",                : O       O\n",
            "but              : O       O\n",
            "i                : O       O\n",
            "was              : O       O\n",
            "waiting          : O       O\n",
            "hours            : O       O\n",
            "and              : O       O\n",
            "nothing          : O       O\n",
            "happened         : O       O\n",
            ".                : O       O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_2UTIftORHs"
      },
      "source": [
        "import openpyxl\n",
        "\n",
        "  \n",
        "wb = openpyxl.Workbook()\n",
        "\n",
        "sheet = wb.active\n",
        "sheet.append([\"Word\",\"Pred\"])\n",
        "\n",
        "for i in range(X_train.shape[0]):\n",
        "  y_predicted =model.predict(np.array([X_train[i]]))\n",
        "  y_predicted = np.argmax(y_predicted, axis=-1) \n",
        "  true = np.argmax(y_train[i], -1)\n",
        "\n",
        "\n",
        "# Visualization\n",
        "  #print(\"Sample number {} of {} (Test Set)\".format(i, X.shape[0]))\n",
        "# Visualization\n",
        "  #print(\"{:15}||{:5}||{}\".format(\"Word\", \"Pred\", \"t\"))\n",
        "  #print(30 * \"=\")\n",
        "  for w, t, pred in zip(X_train[i], true, y_predicted[0]):\n",
        "    if w != 0: # PAD값은 제외함.\n",
        "        #print(\"{:17}: {:7} {}\".format(index_to_word[w], index_to_ner[t], index_to_ner[pred]))\n",
        "        sheet.append([index_to_word[w],index_to_ner[pred]])\n",
        "wb.save(\"na_pred.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}